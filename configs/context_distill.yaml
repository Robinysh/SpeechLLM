config_optimizers:
  decay_step: 105000
  init_lr: 2.0e-07
  learning_rate: 4.0e-05
  lr_restart_period: 3000
  optimizer: adam
  warmup_step: 3000
  weight_decay: 0
defaults:
  - database: default
  - data_module: soda_distill_implcot
  - model: frozen_context_distill
  - losses: distill
  - callbacks: default
  - trainer: hpu_ddp
  - lightning_module: default
  - logger: default
  - paths: laion
  - _self_
last_ckpt: null
load_optimizer: true
offline: false
project_name: speechllm
run_name: null
train_stage: default
