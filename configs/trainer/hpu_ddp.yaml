_target_: lightning.Trainer
accelerator:
  _target_: lightning_habana.pytorch.accelerator.HPUAccelerator
callbacks:
  - ${callbacks.best_checkpoint_callback}
  - ${callbacks.last_checkpoint_callback}
  - ${callbacks.exception_checkpoint_callback}
  - ${callbacks.lr_callback}
detect_anomaly: false
devices: 1
limit_train_batches: null
limit_val_batches: 25
log_every_n_steps: 100
logger: ${logger}
max_epochs: 99999
num_nodes: 1
num_sanity_val_steps: 0
precision: bf16-mixed
strategy:
  _target_: lightning_habana.pytorch.strategies.HPUDDPStrategy
  ddp_comm_hook: torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook
  gradient_as_bucket_view: true
  static_graph: true
val_check_interval: 800
