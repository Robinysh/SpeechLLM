_target_: lightning.Trainer
accelerator:
  _target_: lightning_habana.pytorch.accelerator.HPUAccelerator
callbacks:
  - ${callbacks.best_checkpoint_callback}
  - ${callbacks.last_checkpoint_callback}
  - ${callbacks.exception_checkpoint_callback}
detect_anomaly: false
devices: 1
limit_train_batches: null
limit_val_batches: 100
log_every_n_steps: 100
logger: ${logger}
max_epochs: 99999
num_nodes: 1
num_sanity_val_steps: 1
precision: bf16-mixed
reload_dataloaders_every_n_epochs: 1
strategy:
  _target_: lightning_habana.pytorch.strategies.HPUDDPStrategy
  ddp_comm_hook: torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook
val_check_interval: 500
