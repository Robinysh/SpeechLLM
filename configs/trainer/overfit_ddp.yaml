_target_: lightning.Trainer
accelerator:
  _target_: lightning_habana.pytorch.accelerator.HPUAccelerator
detect_anomaly: false
devices: 1
check_val_every_n_epoch: null
log_every_n_steps: 10
val_check_interval: 2000
logger: ${logger}
max_epochs: 99999
overfit_batches: 1
precision: bf16-mixed
strategy:
  _target_: lightning_habana.pytorch.strategies.HPUDDPStrategy
  ddp_comm_hook: torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook
  gradient_as_bucket_view: true
  static_graph: true  
enable_checkpointing: false
