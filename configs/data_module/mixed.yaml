batch_size: 1
data_module:
  _target_: speechllm.data.datamodule.BasicDataModule
  train_dataloader_args:
    batch_size: ${data_module.batch_size}
    collate_fn:
      _partial_: true
      _target_: speechllm.data.collate.collate
      tokenizer:
        _target_: speechllm.data.utils.get_tokenizer
        model_fpath: ${paths.pretrained_models}/AnyGPT-chat
        offline: ${offline}
    drop_last: true
    num_workers: ${data_module.num_workers}
    persistent_workers: ${data_module.persistent_workers}
    pin_memory: ${data_module.pin_memory}
    prefetch_factor: ${data_module.prefetch_factor}
  train_dataset: ${data_module.mixed_train_dataset}
  val_dataloader_args:
    batch_size: ${data_module.batch_size}
    collate_fn:
      _partial_: true
      _target_: speechllm.data.collate.collate
      tokenizer:
        _target_: speechllm.data.utils.get_tokenizer
        model_fpath: ${paths.pretrained_models}/AnyGPT-chat
        offline: ${offline}
    drop_last: true
    num_workers: 1
    persistent_workers: ${data_module.persistent_workers}
    pin_memory: ${data_module.pin_memory}
    prefetch_factor: 2
  val_dataset: ${data_module.mixed_val_dataset}
gigaspeech_train_dataset:
  _target_: speechllm.data.gigaspeech.pipeline.gigaspeech_asr_tts_cot_pipeline
  cache_dir: ${paths.dataset_cache}
  database: ${database.train.gigaspeech}
  num_workers: ${data_module.num_preprocess_workers}
  stage: train
gigaspeech_val_dataset:
  _target_: speechllm.data.gigaspeech.pipeline.gigaspeech_asr_tts_cot_pipeline
  cache_dir: ${paths.dataset_cache}
  database: ${database.val.gigaspeech}
  num_workers: ${data_module.num_preprocess_workers}
  stage: val
mixed_train_dataset:
  _target_: datasets.interleave_datasets
  datasets:
    - ${data_module.soda_train_dataset}
    - ${data_module.gigaspeech_train_dataset}
  probabilities:
    - 0.5
    - 0.5
  stopping_strategy: 'all_exhausted'
mixed_val_dataset:
  _target_: datasets.interleave_datasets
  datasets:
    - ${data_module.soda_val_dataset}
    - ${data_module.gigaspeech_val_dataset}
  probabilities:
    - 0.5
    - 0.5
  stopping_strategy: 'all_exhausted'
num_preprocess_workers: 16
num_workers: 16
persistent_workers: false
pin_memory: false
prefetch_factor: 2
soda_train_dataset:
  _target_: speechllm.data.soda.pipeline.soda_tts_cot_pipeline
  cache_dir: ${paths.dataset_cache}
  database: ${database.train.soda}
  num_workers: ${data_module.num_preprocess_workers}
  stage: train
soda_val_dataset:
  _target_: speechllm.data.soda.pipeline.soda_tts_cot_pipeline
  cache_dir: ${paths.dataset_cache}
  database: ${database.val.soda}
  num_workers: ${data_module.num_preprocess_workers}
  stage: val
